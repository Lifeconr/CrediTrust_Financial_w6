{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129a19df",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_recall, context_precision\n",
    "from tqdm import tqdm\n",
    "import os, sys\n",
    "\n",
    "# Add project root to path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from src.rag_pipeline import RAGPipeline\n",
    "\n",
    "# Configuration\n",
    "DATA_PATH = '../data/filtered_and_cleaned_complaints.csv'\n",
    "VECTOR_STORE_PATH = '../vector_store/'\n",
    "rag_pipeline = RAGPipeline(\n",
    "    model_name=\"google/gemma-2b-it\",\n",
    "    embedding_model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    vector_store_path=VECTOR_STORE_PATH\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1546d4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load models and FAISS index\n",
    "rag_pipeline.load_embedding_model()\n",
    "rag_pipeline.load_faiss_index()\n",
    "rag_pipeline.load_llm()\n",
    "rag_pipeline.setup_rag_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b748f48f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Sample test questions and ground truths\n",
    "test_data = [\n",
    "    {\n",
    "        \"question\": \"Why are customers unhappy with BNPL services?\",\n",
    "        \"ground_truth\": \"Customers are unhappy with BNPL due to unclear terms, high interest rates, and issues with payment disputes.\",\n",
    "        \"contexts\": []\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What issues are reported with credit card billing?\",\n",
    "        \"ground_truth\": \"Common issues include unauthorized charges, billing disputes, and delays in resolving complaints.\",\n",
    "        \"contexts\": []\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What problems occur with money transfers?\",\n",
    "        \"ground_truth\": \"Money transfer issues include delayed transfers, incorrect recipient details, and poor customer service.\",\n",
    "        \"contexts\": []\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How do customers describe savings account issues?\",\n",
    "        \"ground_truth\": \"Customers report frozen accounts, unexpected fees, and difficulty accessing funds.\",\n",
    "        \"contexts\": []\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What are common personal loan complaints?\",\n",
    "        \"ground_truth\": \"Personal loan complaints often involve high interest rates, misleading terms, and slow approval processes.\",\n",
    "        \"contexts\": []\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b76dda9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Generate RAG responses\n",
    "rag_responses = []\n",
    "for item in tqdm(test_data, desc=\"Generating RAG responses\"):\n",
    "    answer, _, retrieved_docs = rag_pipeline.query(item[\"question\"])\n",
    "    contexts = [doc['text_content'] for doc in retrieved_docs]\n",
    "    rag_responses.append({\n",
    "        \"question\": item[\"question\"],\n",
    "        \"answer\": answer,\n",
    "        \"ground_truth\": item[\"ground_truth\"],\n",
    "        \"contexts\": contexts\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1368ce8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Convert to Dataset for evaluation\n",
    "eval_dataset = Dataset.from_pandas(pd.DataFrame(rag_responses))\n",
    "print(\"Evaluation dataset prepared:\")\n",
    "print(pd.DataFrame(rag_responses).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e942d5b5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Ragas evaluation\n",
    "class RagasLocalLLM:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "    def generate(self, messages, **kwargs):\n",
    "        prompt = \"\".join([f\"{'Human' if isinstance(m, HumanMessage) else 'AI'}: {m.content}\\n\" for m in messages])\n",
    "        response = self.llm.invoke(prompt)\n",
    "        return LLMResult(generations=[[Generation(text=response)]])\n",
    "\n",
    "ragas_llm = RagasLocalLLM(rag_pipeline.llm)\n",
    "result = evaluate(\n",
    "    eval_dataset,\n",
    "    metrics=[faithfulness, answer_relevancy, context_recall, context_precision],\n",
    "    llm=ragas_llm,\n",
    "    embeddings=rag_pipeline.embeddings\n",
    ")\n",
    "print(\"Ragas Evaluation Results:\")\n",
    "print(result.to_pandas())\n",
    "print(\"Mean Scores:\")\n",
    "print(result.to_pandas().mean(numeric_only=True))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
