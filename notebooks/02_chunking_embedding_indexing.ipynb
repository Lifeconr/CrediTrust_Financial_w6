{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e62bbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import faiss\n",
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Sample data for testing (replace with actual dataset path in production)\n",
    "sample_data = pd.DataFrame({\n",
    "    'Complaint ID': [1001, 1002, 1003],\n",
    "    'Product': ['Credit card', 'Personal loan', 'Savings account'],\n",
    "    'cleaned_narrative': [\n",
    "        'unauthorized transaction appeared account disputed bank resolved issue timely manner',\n",
    "        'applied loan high interest rate disclosed upfront felt misled terms',\n",
    "        'account frozen without notice customer service unresponsive'\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Define paths\n",
    "cleaned_data_path = '../data/filtered_and_cleaned_complaints.csv'\n",
    "vector_store_dir = '../vector_store/'\n",
    "faiss_index_path = os.path.join(vector_store_dir, 'faiss_index.bin')\n",
    "metadata_path = os.path.join(vector_store_dir, 'metadata.json')\n",
    "\n",
    "# Load dataset (use sample_data for testing)\n",
    "df_cleaned = sample_data if os.path.exists(cleaned_data_path) else pd.read_csv(cleaned_data_path, encoding='utf-8')\n",
    "print(f\"Loaded dataset shape: {df_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbe01b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    add_start_index=True\n",
    ")\n",
    "\n",
    "# Prepare documents for splitting\n",
    "documents = [\n",
    "    {\n",
    "        \"page_content\": row['cleaned_narrative'],\n",
    "        \"metadata\": {\n",
    "            \"product\": row['Product'],\n",
    "            \"complaint_id\": row['Complaint ID']\n",
    "        }\n",
    "    } for _, row in df_cleaned.iterrows() if pd.notna(row['cleaned_narrative']) and row['cleaned_narrative'].strip()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf46168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split documents into chunks\n",
    "chunks = text_splitter.create_documents(\n",
    "    [d[\"page_content\"] for d in documents],\n",
    "    metadatas=[d[\"metadata\"] for d in documents]\n",
    ")\n",
    "print(f\"Created {len(chunks)} chunks\")\n",
    "\n",
    "# Store chunks in DataFrame\n",
    "chunks_df = pd.DataFrame([\n",
    "    {\n",
    "        'chunk_content': chunk.page_content,\n",
    "        'product': chunk.metadata['product'],\n",
    "        'complaint_id': chunk.metadata['complaint_id']\n",
    "    } for chunk in chunks\n",
    "])\n",
    "print(f\"Chunks DataFrame shape: {chunks_df.shape}\")\n",
    "print(\"Sample chunks:\")\n",
    "print(chunks_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87624a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize chunk length distribution\n",
    "chunk_lengths = [len(chunk.page_content) for chunk in chunks]\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(chunk_lengths, bins=20, kde=True, color='purple')\n",
    "plt.title('Distribution of Chunk Lengths')\n",
    "plt.xlabel('Chunk Length (characters)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "print(f\"Chunk length stats: Min={np.min(chunk_lengths)}, Max={np.max(chunk_lengths)}, Mean={np.mean(chunk_lengths):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d625746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings_np = model.encode(chunks_df['chunk_content'].tolist(), show_progress_bar=True, convert_to_numpy=True)\n",
    "print(f\"Embeddings shape: {embeddings_np.shape}\")\n",
    "\n",
    "# Initialize and populate FAISS index\n",
    "dimension = embeddings_np.shape[1]\n",
    "faiss_index = faiss.IndexFlatIP(dimension)\n",
    "faiss_index.add(embeddings_np)\n",
    "print(f\"FAISS index contains {faiss_index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bd29c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save FAISS index and metadata\n",
    "os.makedirs(vector_store_dir, exist_ok=True)\n",
    "faiss.write_index(faiss_index, faiss_index_path)\n",
    "chunks_df.to_json(metadata_path, orient='records', indent=4)\n",
    "print(f\"Saved FAISS index to {faiss_index_path} and metadata to {metadata_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459cea80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
